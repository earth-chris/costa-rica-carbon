{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling notebook\n",
    "\n",
    "Here, we estimate country-wide tree height scores, using LVIS-derived height scores as the response variable and landsat- and palsar-derived remote sensing data as covariates.\n",
    "\n",
    "Steps:\n",
    "\n",
    "* Extract co-aligned pixel values for the following datasets\n",
    "    * LVIS lidar\n",
    "    * Landsat fractional cover\n",
    "    * PALSAR backscatter\n",
    "    * CCB tree cover\n",
    "* Perform hyperparameter tuning from a small subset of data\n",
    "* Train a model on the full dataset using the best-fit hyperparameters\n",
    "* Apply that model to the full country's covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# set to allow re-loading modules while in-dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import ccb\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import multiclass\n",
    "from sklearn import calibration\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "# set the seed\n",
    "random.seed(1985)\n",
    "\n",
    "# set and create directories\n",
    "data_dir = 'data'\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "# set data paths\n",
    "covariate_path = os.path.join(data_dir, 'covariate-stack.tif')\n",
    "response_path = os.path.join(data_dir, 'tree-height-lvis-masked.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading input data\n",
    "We'll read data from two paths: the response data (LVIS lidar) and the covariate data (Landsat/PALSAR/tree cover). Each of these datasets were created and masked to the same good-data locations per the `data-processing` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading response variable data into memory\n",
      "2061698 good-data values found\n"
     ]
    }
   ],
   "source": [
    "# read the response data into memory and subset to the good-data values\n",
    "print(\"Reading response variable data into memory\")\n",
    "r = ccb.read.raster(response_path)\n",
    "r.read_all()\n",
    "gd = r.data != r.no_data\n",
    "gd_count = gd.sum()\n",
    "response = r.data[gd]\n",
    "\n",
    "# clear the ref\n",
    "r.data = None\n",
    "r = None\n",
    "\n",
    "print(\"{n} good-data values found\".format(n = gd_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading covariate band: 1\n",
      "Reading covariate band: 2\n",
      "Reading covariate band: 3\n",
      "Reading covariate band: 4\n",
      "Reading covariate band: 5\n",
      "Reading covariate band: 6\n"
     ]
    }
   ],
   "source": [
    "# read the covariate data into memory, band-by-band, and do the same subset\n",
    "c = ccb.read.raster(covariate_path)\n",
    "covariates = np.zeros((gd_count, c.nb))\n",
    "for band_number in range(c.nb):\n",
    "    print(\"Reading covariate band: {n}\".format(n = band_number + 1))\n",
    "    c.read_band(band_number + 1)\n",
    "    covariates[:, band_number] = c.data[gd]\n",
    "    c.data = None\n",
    "    \n",
    "c = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "We'll create a small subset of data that we'll use to run a grid search for model hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlarge, xsmall, ylarge, ysmall = model_selection.train_test_split(\n",
    "    covariates, \n",
    "    response, \n",
    "    test_size=0.08\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed: 27.1min\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed: 85.8min\n",
      "[Parallel(n_jobs=2)]: Done 729 out of 729 | elapsed: 667.9min finished\n"
     ]
    }
   ],
   "source": [
    "# run model tuning for gradient boosting regression\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# create the tuning object\n",
    "gbm = ccb.ml.tuner(\n",
    "    xsmall, \n",
    "    ysmall, \n",
    "    scoring = scoring,\n",
    "    n_splits = 3,\n",
    "    cv = model_selection.KFold(3),\n",
    "    n_jobs = 2,\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "gbm.GradientBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r**2 on tuning data : 0.586\n",
      "rmse on tuning data : 56.363\n"
     ]
    }
   ],
   "source": [
    "# save the best model\n",
    "gbm_tuned_path = os.path.join(model_dir, 'gbm-tuned.pck')\n",
    "with open(gbm_tuned_path, 'wb') as f:\n",
    "    pickle.dump(gbm.best_estimator, f)\n",
    "\n",
    "# report performance on tuning data\n",
    "ypred = gbm.best_estimator.predict(xsmall)\n",
    "r2 = metrics.r2_score(ysmall, ypred)\n",
    "rmse = metrics.mean_squared_error(ysmall, ypred)\n",
    "print('r**2 on tuning data : {:0.3f}'.format(r2))    \n",
    "print('rmse on tuning data : {:0.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Now that we've found the best hyperparameters to train the model, run a 3-fold cross-validation then use the full dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
